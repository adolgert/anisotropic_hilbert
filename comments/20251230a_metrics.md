## Number of clusters

### Description

Based on my reading of this paper by Moon, Jagadish, Faloutsos, and Saltz, here's a summary of the metrics and methodology:
#### Primary Metric: Number of Clusters
The paper uses the average number of clusters (or "continuous runs") as the main metric to characterize the clustering property of space-filling curves. A cluster is defined as "a group of grid points inside the query that are consecutively connected by a mapping (or a curve)" (Definition 1.1). This metric is chosen because:
It corresponds to the number of non-consecutive disk accesses when data is stored according to the curve's ordering
Fewer clusters = better locality preservation = fewer disk seeks
#### Key Results
**Asymptotic formula (Theorem 1):** For a d-dimensional rectilinear polyhedron with total surface area S_q:

N_d = S_q / (2d)
For hypercubes of side length s: N_d = s^(d-1)
#### Dimensions Analyzed
2D: Both exact closed-form formulas and exhaustive simulations on 1024×1024 and 32K×32K grids
3D: Statistical simulations on 32K×32K×32K grids
Up to 10D: Statistical simulations for hypercubic queries
#### Baselines / Comparisons
The paper compares three space-filling curves:
Hilbert curve (the focus)
Z-curve (also called Morton ordering)
Gray-coded curve
**Key finding:** The Hilbert curve outperforms both alternatives:
For 2D square queries: Hilbert achieved up to 43% improvement in worst-case and 48% in average-case over competitors
For 3D spherical queries: up to 28-31% improvement over z-curve, 18-22% over Gray-coded curve
The z-curve's formula (from prior work) gives approximately 1/3 of perimeter + 2/3 of unfavored side length, versus Hilbert's 1/4 of perimeter in 2D.

### Plan

 * Hamilton and mine
 * 2D, 3D, 4D
 * even dimensions and skewed dimensions
 * total size N similar for all of them.
 * Hypercubes of sizes smaller than the smallest dimension?
 * Measure... all of them if N is small,
   random sample if N is large.

## Box-to-curve Ratio Metric Analysis

Based on my review of both Haverkort papers, here's a summary of the BCR metric:
### What is BCR?
BCR (Box-to-Curve Ratio) measures the efficiency of space-filling curves for spatial indexing applications like R-trees. It is defined as:
BCR of a section f([a,b]) = (volume of minimum axis-aligned bounding box of f([a,b])) / (volume covered by f([a,b]))
In other words, it measures how much "wasted space" exists in the bounding box compared to the actual curve section volume. A BCR of 1 would be ideal (bounding box equals curve volume), but this is generally impossible.
### Key Results
Result	Value
Upper bound for hyperorthogonal well-folded curves	BCR ≤ 4 (independent of dimension d)
Lower bound for face-continuous curves	BCR ≥ 4 − 16/(2^d + 3)
Lower bound for non-face-continuous curves	BCR ≥ 4 − 4/2^d
Butz-Moore curves	BCR = Ω(2^(d/2)) — exponentially worse
### Definition (Formal)
From the hyperorthogonal paper (Section 1.2):
For any 0 ≤ a ≤ b ≤ 1, let f([a,b]) denote the section of the space-filling curve f from f(a) to f(b). The box-to-curve ratio of a section f([a,b]) is: bcr(f([a,b])) = |bbox(f([a,b]))| / |f([a,b])| where |bbox(·)| is the volume of the minimum axis-aligned bounding box and |f([a,b])| is the d-dimensional Lebesgue measure (volume) of the curve section.
### Baseline/Optimal Value
Theoretical optimal: BCR = 1 (bounding box equals curve volume)
Practical lower bound: BCR ≥ 2 for 2D curves based on rectangular grids (proven in the 2010 paper)
Asymptotic lower bound: BCR ≥ 4 − O(1/2^d) for any curve using Hamiltonian paths on hypercubic grids
### Dimensions Analyzed
Both papers analyze dimensions d = 2 through d = 6 with computed values, and provide asymptotic bounds for d ≥ 7. Table 1 from the hyperorthogonal paper:
Curve	d=2	d=3	d=4	d=5	d=6	d≥7
Lower bound (face-continuous)	2.00	2.54	3.15	3.54	3.76	4−16/(2^d+3)
Self-similar hyperorthogonal, f(0)=(0,...,0)	2.40	3.11	3.53	3.76	3.88	≤4
Self-similar hyperorthogonal, f(0)=(1/3,...,1/3,0)	—	3.14	3.67	3.83	3.92	≤4
Butz-Moore	2.40	3.11	4.74	7.08	10.65	Ω(2^(d/2))
Comparisons
Hyperorthogonal well-folded curves vs. Butz-Moore: The new curves maintain BCR ≤ 4 in all dimensions, while Butz-Moore curves degrade exponentially (Ω(2^(d/2)))
For d=2: Hilbert's curve achieves BCR = 2.40, and the βΩ-curve achieves BCR = 2.22 (best known)
Key insight: Traditional locality measures (like WL₂) don't predict bounding-box quality well. Curves with good locality (like Sierpiński-Knopp with WL₂=4) can have poor BCR (3.0), while curves with moderate locality can have excellent BCR.
### Extent Sizes in Haverkort Papers
2010 Paper ("Locality and bounding-box quality of two-dimensional space-filling curves")
Used random sampling rather than exhaustive grid testing
Sample sizes: 500 to 18,000 points per test (log-uniform distribution)
Not a fixed grid like 32k × 32k
2015 Paper ("Hyperorthogonal well-folded Hilbert curves")
Uses approximating curves A_k with 2^(d·k) total points
For 2D (d=2): tested with k = 2 to 6, meaning:
k=2: 4×4 = 16 points
k=3: 8×8 = 64 points
k=4: 16×16 = 256 points
k=5: 32×32 = 1,024 points
k=6: 64×64 = 4,096 points
For higher dimensions (d=3 to 6), k was limited to 2 ≤ k ≤ 12/d:
d=3: up to 16×16×16 = 4,096 points
d=4: up to 8×8×8×8 = 4,096 points
d=5: up to 4×4×4×4×4 = 1,024 points
d=6: up to 4×4×4×4×4×4 = 4,096 points

**Answer to your question:** No, they did not use 32k × 32k grids. The largest 2D grid was 64×64 (4,096 points). The theoretical bounds they prove hold for arbitrary k (and thus arbitrarily large grids), but computational verification was done on these smaller sizes.

## Metrics in Gotsman & Lindenbaum (1996) "On the Metric Properties of Discrete Space-Filling Curves"
### Two Locality Metrics: L₁ and L₂
L₁(C) - Worst-case locality (maximum ratio) $$L_1(C) = \max_{i,j \in [N^m], i < j} \frac{d(C(i), C(j))^m}{|i - j|}$$ Measures the worst case where points close along the curve are far apart in Euclidean space. L₂(C) - Best-case locality (minimum ratio) $$L_2(C) = \min_{i,j \in [N^m], i < j} \frac{d(C(i), C(j))^m}{|i - j|}$$ Measures the best case (whether curve distance predicts Euclidean distance).
### Key Results
Curve Type	L₁ Bound	L₂ Bound
Any curve (lower bound)	L₁(C) ≥ 2^m - 1	L₂(C) = O(N^(1-m)) → 0
Raster scan	L₁ = Ω(N^(m-1)) — poor	—
Hilbert (general m-D)	L₁ ≤ (m+3)^(m/2) · 2^m	—
Hilbert 2D	6(1 - O(2^(-k))) ≤ L₁ ≤ 6⅔	—
Hilbert 3D	L₁ ≤ 23 (by simulation)	—
### Baseline
* Lower bound for any curve: L₁ ≥ 2^m - 1 (Theorem 1)
* For 2D: L₁ ≥ 3 (improved to 3.25 by exhaustive search)
* L₂ always goes to 0 as N→∞ (Theorem 2) — curve distance cannot reliably predict Euclidean distance
### Dimensions Analyzed
* General m-dimensional theory
* Specific results for 2D and 3D
* Grid sizes: N = 2^k (powers of 2 for Hilbert curves)
### Comparisons
*Hilbert curve: L₁ ≈ 6 for 2D — near optimal (lower bound is 3.25)
* Raster scan: L₁ = Ω(N^(m-1)) — grows with grid size, very poor
* Hilbert achieves constant L₁ independent of grid size
### Key Insight
The exponent m in the metrics normalizes for dimensionality. The paper proves Hilbert curves are near-optimal for preserving locality — points close along the curve stay within a bounded Euclidean distance, with that bound being a small constant (≈6 in 2D) regardless of grid size.

## Gotsman & Lindenbaum Locality Ratio

The two locality metrics from Gotsman & Lindenbaum are: L₁(C) - Maximum locality ratio:

L₁(C) = max over all pairs (i,j) of:  d(C(i), C(j))^m / |i - j|
L₂(C) - Minimum locality ratio:

L₂(C) = min over all pairs (i,j) of:  d(C(i), C(j))^m / |i - j|
Where:
i and j are positions along the curve (indices in [N^m])
C(i) and C(j) are the corresponding grid points
d(C(i), C(j)) is the Euclidean distance between those grid points
|i - j| is the distance along the curve
m is the dimension
In plain English:
L₁ finds the worst case: the pair of points closest along the curve but farthest apart in Euclidean space
L₂ finds the best case: the pair closest in Euclidean space relative to their curve distance
The exponent m normalizes for dimension since max Euclidean distance is O(N) while max curve distance is O(N^m).

# Example Metrics

Your draft explicitly frames the comparison as: existing compact Hilbert indices (CHIs) for *unequal* side lengths can have “seam jumps” (consecutive indices mapping to nonadjacent lattice points), while your construction guarantees lattice continuity (adjacency) even on dyadic hyperrectangles.    The draft also says the experimental section is intended to report “locality metrics” and “bounding-box quality” and compare to existing CHIs.   This matches what an *Algorithmica*-style reader would expect: a small set of “standard” locality / bounding-box / clustering tables (from prior curve-evaluation literature) plus one or two tables that directly quantify your new guarantee (no seam jumps) on rectangular domains. 

Below is a concrete set of **sample tables with made-up values** (clearly labeled) that show: what to report, what domain extents to use, what is exhaustive vs sampled, and what baseline numbers to cite from the literature.

---

## Metric families and the tables I would look for

### A. “Does the curve actually have unit-step adjacency?”

This is the core novelty. Your paper defines lattice continuity as a bijection whose consecutive indices map to adjacent lattice points.   Existing CHIs for unequal side lengths need not have this property. 

**Tables to include:**

1. **Step-distance / seam-jump table** (exhaustive, O(N)).
2. **Seam-jump “where do they occur” table** (optional, but makes the story concrete: jumps concentrate at axis-activation boundaries).

### B. “Does adjacency improvement matter for standard curve-quality metrics?”

Many standard metrics in the literature are defined for (continuous) space-filling curves where consecutive cells touch. For example, in the discrete setting Gotsman–Lindenbaum explicitly define a discrete SFC to have unit steps (d(C(i),C(i+1))=1).  

Haverkort & van Walderveen define widely used **worst-case locality** measures (WL_r) and **bounding-box** measures (WBA), (WBP).  

**Tables to include:**
3) **Worst-case locality / bounding-box constants** (or finite-order estimates), including a clear “metric applicability” note if Hamilton-CHI is discontinuous on rectangles.
4) **Block bounding-box statistics** (application-facing; simulates packing consecutive points into blocks as in R-tree-like indexing).

### C. “What happens for range queries?”

The classic range-query quality metric is **clustering**: how many disjoint 1D index intervals a query window turns into. The clustering paper you attached gives both asymptotic theory and experimental methodology (exhaustive for small, random sampling for large/high-dimensional), and relates average clusters to perimeter/surface area.  

**Tables to include:**
5) **Average clustering for a few query shapes/sizes** on rectangular domains (sampled), reporting mean / tail / max.

### D. “Do the Gray-code variants matter?”

Your draft explicitly notes there is *no cross-dimensional Gray-code constraint* (Gray codes and tables are “level-local”), so comparing a small set of Gray-code variants is reasonable. 

**Table strategy:**

* Use **BRGC** to isolate “continuity vs seam jumps” (closest to standard Hilbert behavior).
* Add **one “best-of-class”** Gray code (e.g., your balanced variant) to show your method supports “quality tuning” beyond adjacency.
* Optionally add **one “stress-test”** (random Gray code) to show variability.

---

## Sample tables with made-up values

All numbers below are **fabricated** to illustrate *table structure, columns, and expected trends*. They are not computed results.

### Table 0 — Curves / variants compared

(Helps the reader immediately see what differs.)

| Label in tables   | Construction                                                                | Gray code               | Lattice-continuous on rectangles? | Notes                                                                                                     |
| ----------------- | --------------------------------------------------------------------------- | ----------------------- | --------------------------------: | --------------------------------------------------------------------------------------------------------- |
| Hamilton-CHI      | Compact Hilbert index preserving order of a Hilbert curve on a padded cube  | BRGC (standard Hilbert) |          No (can have seam jumps) | Defined by order-preservation (H'(p)!<!H'(q)\iff H(p)!<!H(q)) on (D\subseteq P').                         |
| LC-CHI (BRGC)     | Your lattice-continuous CHI                                                 | BRGC                    |                               Yes | Lattice continuity guaranteed (Thm 6.4).                                                                  |
| LC-CHI (Balanced) | Your method                                                                 | balanced                |                               Yes | Shows “best-known Gray code” variant in your implementation set. (Variant choice allowed by Remark 6.1.)  |
| LC-CHI (Random)   | Your method                                                                 | random                  |                               Yes | Sensitivity / robustness check.                                                                           |

---

### Table 1 — Domain instances

(Explicitly targets **non-square** cases, since that’s where the methods diverge.)

Suggestion: keep **total bits (M=\sum m_i)** constant across rows so “same number of points” comparisons are clean.

| Domain ID | (m=(m_x,m_y)) |  Grid size | Aspect ratio | Total bits (M) | Points (N=2^M) |
| --------: | ------------: | ---------: | -----------: | -------------: | -------------: |
|        D0 |        (9, 9) |  512 × 512 |        1 : 1 |             18 |        262,144 |
|        D1 |       (11, 7) | 2048 × 128 |       16 : 1 |             18 |        262,144 |
|        D2 |       (12, 6) |  4096 × 64 |       64 : 1 |             18 |        262,144 |
|        D3 |       (13, 5) |  8192 × 32 |      256 : 1 |             18 |        262,144 |

---

### Table 2 — Lattice continuity and seam-jump metrics

This is the headline table for your paper’s key improvement.

**Computation:** exhaustive enumeration of the full curve on the domain; compute L1 distance between successive points. (No random sampling.)

Definitions to report:

* (J): number of steps with ( |p_{t+1}-p_t|_1 > 1) (“seam jumps”).
* (\max \Delta_1): max L1 step size.
* mean (\Delta_1): mean L1 step size.
* jump rate (J/(N-1)).

| Domain        | Curve             | (J) (non-adjacent steps) | Jump rate (%) | (\max \Delta_1) | mean (\Delta_1) |
| ------------- | ----------------- | -----------------------: | ------------: | --------------: | --------------: |
| D0 (512×512)  | Hamilton-CHI      |                        0 |        0.0000 |               1 |           1.000 |
|               | LC-CHI (BRGC)     |                        0 |        0.0000 |               1 |           1.000 |
|               | LC-CHI (Balanced) |                        0 |        0.0000 |               1 |           1.000 |
| D1 (2048×128) | Hamilton-CHI      |                       15 |        0.0057 |             128 |           1.007 |
|               | LC-CHI (BRGC)     |                        0 |        0.0000 |               1 |           1.000 |
|               | LC-CHI (Balanced) |                        0 |        0.0000 |               1 |           1.000 |
| D2 (4096×64)  | Hamilton-CHI      |                       63 |        0.0240 |              64 |           1.015 |
|               | LC-CHI (BRGC)     |                        0 |        0.0000 |               1 |           1.000 |
|               | LC-CHI (Balanced) |                        0 |        0.0000 |               1 |           1.000 |
| D3 (8192×32)  | Hamilton-CHI      |                      255 |        0.0973 |              32 |           1.030 |
|               | LC-CHI (BRGC)     |                        0 |        0.0000 |               1 |           1.000 |
|               | LC-CHI (Balanced) |                        0 |        0.0000 |               1 |           1.000 |

**Why this table is persuasive even if jump rates are small:** worst-case locality/bounding-box metrics are *suprema*; a small number of seam jumps can blow up “worst-case” measures (next table).

---

### Table 3 — Worst-case locality and bounding-box quality

This is the “Algorithmica reader” table: it ties your new guarantee to standard measures.

Haverkort defines (WL_r) and (WBA/WBP) as worst-case (supremum) ratios over contiguous curve sections, in the limit of refinement.   In their Table 1, the classic **Hilbert** order on the unit square has (WL_\infty=6), (WL_2=6), (WL_1=9), (WBA=2.400), (WBP=2.400); **Z-order** has (\infty) for several worst-case measures. 

In your paper you can either:

* report these measures as **“finite-order estimates”** at fixed (m), or
* report **“∞ / unbounded”** for curves that violate adjacency on rectangles (and explain why).

Below is a *hybrid* style: show a finite-order estimate plus an “unbounded as refinement increases” marker.

| Domain       | Curve             |           (WL_\infty) |                (WL_2) |                (WL_1) |          (WBA) |                 (WBP) |
| ------------ | ----------------- | --------------------: | --------------------: | --------------------: | -------------: | --------------------: |
| D0 (512×512) | Hamilton-CHI      |                   6.0 |                   6.0 |                   9.0 |           2.40 |                  2.40 |
|              | LC-CHI (BRGC)     |                   6.0 |                   6.0 |                   9.0 |           2.40 |                  2.40 |
|              | LC-CHI (Balanced) |                   5.8 |                   5.9 |                   8.7 |           2.30 |                  2.35 |
|              | LC-CHI (Random)   |                   7.2 |                   7.4 |                  10.5 |           3.10 |                  3.40 |
| D2 (4096×64) | Hamilton-CHI      | 1.28×10^5 (unbounded) | 1.29×10^5 (unbounded) | 1.31×10^5 (unbounded) | 64 (unbounded) | 3.28×10^4 (unbounded) |
|              | LC-CHI (BRGC)     |                   6.5 |                   6.6 |                   9.8 |           2.80 |                  3.00 |
|              | LC-CHI (Balanced) |                   6.3 |                   6.4 |                   9.4 |           2.60 |                  2.85 |
|              | LC-CHI (Random)   |                   7.9 |                   8.1 |                  11.4 |           3.90 |                  4.30 |
| D3 (8192×32) | Hamilton-CHI      | 1.23×10^5 (unbounded) | 1.24×10^5 (unbounded) | 1.26×10^5 (unbounded) | 32 (unbounded) | 3.28×10^4 (unbounded) |
|              | LC-CHI (BRGC)     |                   6.8 |                   6.9 |                  10.2 |           3.00 |                  3.20 |
|              | LC-CHI (Balanced) |                   6.6 |                   6.7 |                   9.8 |           2.80 |                  3.00 |
|              | LC-CHI (Random)   |                   8.3 |                   8.6 |                  12.0 |           4.50 |                  5.00 |

Notes you’d put under this table:

* (WL_r), (WBA), (WBP) are from the standard definitions. 
* For discontinuous orders, worst-case measures can be infinite/unbounded (Haverkort’s Table 1 shows this explicitly for Z-order). 
* Your construction guarantees lattice continuity on unequal side lengths (Theorem 6.4), so these worst-case measures become meaningful again. 

#### Actual Metrics from Experiments:
  Files Created

  1. src/haverkort_metrics.h - Header with metric types and function declarations
  2. src/haverkort_metrics.c - Main implementation with:
    - Exhaustive O(N²) enumeration for small domains
    - Random sampling for large domains
    - CSV output for all metrics

  Key Metrics Computed

  - WL_∞ (Chebyshev locality)
  - WL_2 (Euclidean locality)
  - WL_1 (Manhattan locality)
  - WBA (Bounding box area ratio)
  - WBP (Bounding box perimeter ratio)

  Results Summary

  | Domain   | Aspect | Hamilton WL_∞ | LC-CHI WL_∞ | Hamilton WBA | LC-CHI WBA |
  |----------|--------|---------------|-------------|--------------|------------|
  | 512×512  | 1:1    | 4.75          | 4.75        | 2.29         | 2.29       |
  | 2048×128 | 16:1   | 442           | 4.75        | 435          | 2.29       |
  | 4096×64  | 64:1   | 384           | 7.62        | 416          | 2.29       |
  | 8192×32  | 256:1  | 512           | 26.2        | 528          | 2.33       |

  The results demonstrate:
  - Square domains: Both algorithms produce identical metrics (as expected)
  - Rectangular domains: Hamilton's metrics explode (93× worse at 16:1 aspect ratio) while LC-CHI stays bounded

---

### Table 4 — Practical block bounding-box statistics

This is the “indexing application” table. It is more intuitive than (WBA/WBP) and more directly tied to how SFCs are used to build blocks (e.g., R-tree leaves). Haverkort motivates this style of evaluation by grouping consecutive points into blocks and measuring block bounding boxes. 

**Computation:** deterministic (no sampling) if you just partition the full curve into consecutive blocks of size (B).
Report both average and max because seam jumps tend to show up in the max/tails.

Example: domain D3 (8192×32).

| Curve             | Block size (B) | mean bbox area / (B) | p95 bbox area / (B) | max bbox area / (B) | max bbox perimeter / (\sqrt{B}) |
| ----------------- | -------------: | -------------------: | ------------------: | ------------------: | ------------------------------: |
| Hamilton-CHI      |             32 |                 1.25 |                2.10 |                32.0 |                            18.5 |
| LC-CHI (BRGC)     |             32 |                 1.24 |                2.00 |                 4.8 |                             6.2 |
| LC-CHI (Balanced) |             32 |                 1.18 |                1.85 |                 4.0 |                             5.8 |
| LC-CHI (Random)   |             32 |                 1.45 |                2.60 |                 9.5 |                             8.9 |
| Hamilton-CHI      |           1024 |                 1.46 |                2.30 |                12.0 |                             9.2 |
| LC-CHI (BRGC)     |           1024 |                 1.45 |                2.20 |                 5.0 |                             6.8 |
| LC-CHI (Balanced) |           1024 |                 1.38 |                2.10 |                 4.5 |                             6.5 |
| LC-CHI (Random)   |           1024 |                 1.90 |                2.85 |                18.0 |                            10.3 |

(Again: fabricated values, but the pattern is what you’d want—Hamilton’s max/tails much worse due to a few discontinuities, while LC-CHI variants keep max/tails controlled.)

---

### Table 5 — Range-query clustering on rectangular domains

This is the “range query” table. It demonstrates practical impact beyond “adjacent steps”.

The clustering paper reports that clustering depends strongly on perimeter; in 2D, the asymptotic average number of clusters is one quarter of the query rectangle perimeter.   It also describes methodology: exhaustive enumeration for small grids; random sampling for larger/high-dimensional due to exponential query positions; and (in one experiment) 200 random queries per size/shape. 

Below: a table layout that makes sense for your rectangular-domain focus. (Made-up numbers.)

**Setup:** domain D2 (4096×64). For each query shape, sample (Q=10{,}000) random placements uniformly (or use exhaustive sliding-window when feasible).

| Curve             | Query window (cells) | Mean clusters | p95 clusters | Max clusters |   Sampling |
| ----------------- | -------------------: | ------------: | -----------: | -----------: | ---------: |
| Hamilton-CHI      |                  4×4 |          4.08 |            6 |           14 | 10k random |
| LC-CHI (BRGC)     |                  4×4 |          4.06 |            6 |           10 | 10k random |
| LC-CHI (Balanced) |                  4×4 |          4.02 |            6 |           10 | 10k random |
| LC-CHI (Random)   |                  4×4 |          4.30 |            7 |           18 | 10k random |
| Hamilton-CHI      |                16×16 |          16.5 |           25 |           60 | 10k random |
| LC-CHI (BRGC)     |                16×16 |          16.2 |           24 |           40 | 10k random |
| LC-CHI (Balanced) |                16×16 |          15.9 |           23 |           38 | 10k random |
| LC-CHI (Random)   |                16×16 |          17.5 |           28 |           70 | 10k random |
| Hamilton-CHI      |                 64×4 |          35.0 |           55 |          120 | 10k random |
| LC-CHI (BRGC)     |                 64×4 |          34.1 |           50 |           80 | 10k random |
| LC-CHI (Balanced) |                 64×4 |          33.5 |           48 |           75 | 10k random |
| LC-CHI (Random)   |                 64×4 |          38.0 |           60 |          140 | 10k random |

---

## A small “literature baseline” table you can cite (not made up)

This is useful because it anchors your reported measures to known values for classic 2D curves.

### Baseline 1: worst-case locality + bounding box (Haverkort & van Walderveen)

From their Table 1:

* Hilbert: (WL_\infty=6), (WL_2=6), (WL_1=9), (WBA=2.400), (WBP=2.400)
* Z-order: (WL_\infty, WL_2, WL_1) reported as (\infty); (WBA) and (WBP) as (\infty) (worst-case)
  These are exactly the kind of numbers referees recognize. 

### Baseline 2: clustering for a tiny query window (Jagadish, reported in clustering paper)

For a 2×2 query window, Jagadish reports average clusters: Z-curve 2.625, Gray-coded curve 2.5, Hilbert 2. 

---

## Sampling vs exhaustive: what to write in captions

A reviewer will look for this level of clarity:

* **Adjacency / seam-jump metrics (Table 2):** *exhaustive*, single pass over all (N) points, O(N). No sampling.
* **Worst-case (WL_r), (WBA), (WBP) (Table 3):** if you want *true worst-case constants* in the Haverkort sense, exact computation is nontrivial; they use an approximation algorithm to compute tight bounds. 
  If you don’t want to implement that algorithm, state that you report **finite-order estimates** (e.g., worst over sampled sections, or worst over all block-aligned sections) and describe the sampling scheme.
* **Block bounding boxes (Table 4):** *exhaustive* if you partition deterministically into blocks; optionally also report random-block subdivisions (Haverkort estimates average total measures by random sampling of subdivisions). 
* **Clustering (Table 5):** exhaustive only for small grids; otherwise random sampling because the number of possible window positions grows as ((N-k+1)^d). 

---

## Minimal recommendation on “how many variants” to include

If the goal is to keep the section focused on *adjacency*, the most convincing subset is:

1. **Hamilton-CHI** (baseline).
2. **LC-CHI (BRGC)** (controls for “standard Hilbert-like” behavior; isolates the effect of lattice continuity).
3. **LC-CHI (Balanced)** (one high-quality alternative Gray code to show the method supports “variant tuning”).

A random Gray code is optional; it is mainly useful if you expect a reviewer to ask “how sensitive is this to the Gray code choice?” Your draft explicitly supports independent Gray code choices across levels/dimensions, so this is easy to justify. 
