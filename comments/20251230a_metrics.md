## Number of clusters

### Description

Based on my reading of this paper by Moon, Jagadish, Faloutsos, and Saltz, here's a summary of the metrics and methodology:
#### Primary Metric: Number of Clusters
The paper uses the average number of clusters (or "continuous runs") as the main metric to characterize the clustering property of space-filling curves. A cluster is defined as "a group of grid points inside the query that are consecutively connected by a mapping (or a curve)" (Definition 1.1). This metric is chosen because:
It corresponds to the number of non-consecutive disk accesses when data is stored according to the curve's ordering
Fewer clusters = better locality preservation = fewer disk seeks
#### Key Results
**Asymptotic formula (Theorem 1):** For a d-dimensional rectilinear polyhedron with total surface area S_q:

N_d = S_q / (2d)
For hypercubes of side length s: N_d = s^(d-1)
#### Dimensions Analyzed
2D: Both exact closed-form formulas and exhaustive simulations on 1024×1024 and 32K×32K grids
3D: Statistical simulations on 32K×32K×32K grids
Up to 10D: Statistical simulations for hypercubic queries
#### Baselines / Comparisons
The paper compares three space-filling curves:
Hilbert curve (the focus)
Z-curve (also called Morton ordering)
Gray-coded curve
**Key finding:** The Hilbert curve outperforms both alternatives:
For 2D square queries: Hilbert achieved up to 43% improvement in worst-case and 48% in average-case over competitors
For 3D spherical queries: up to 28-31% improvement over z-curve, 18-22% over Gray-coded curve
The z-curve's formula (from prior work) gives approximately 1/3 of perimeter + 2/3 of unfavored side length, versus Hilbert's 1/4 of perimeter in 2D.

### Plan

 * Hamilton and mine
 * 2D, 3D, 4D
 * even dimensions and skewed dimensions
 * total size N similar for all of them.
 * Hypercubes of sizes smaller than the smallest dimension?
 * Measure... all of them if N is small,
   random sample if N is large.

## Box-to-curve Ratio Metric Analysis

Based on my review of both Haverkort papers, here's a summary of the BCR metric:
### What is BCR?
BCR (Box-to-Curve Ratio) measures the efficiency of space-filling curves for spatial indexing applications like R-trees. It is defined as:
BCR of a section f([a,b]) = (volume of minimum axis-aligned bounding box of f([a,b])) / (volume covered by f([a,b]))
In other words, it measures how much "wasted space" exists in the bounding box compared to the actual curve section volume. A BCR of 1 would be ideal (bounding box equals curve volume), but this is generally impossible.
### Key Results
Result	Value
Upper bound for hyperorthogonal well-folded curves	BCR ≤ 4 (independent of dimension d)
Lower bound for face-continuous curves	BCR ≥ 4 − 16/(2^d + 3)
Lower bound for non-face-continuous curves	BCR ≥ 4 − 4/2^d
Butz-Moore curves	BCR = Ω(2^(d/2)) — exponentially worse
### Definition (Formal)
From the hyperorthogonal paper (Section 1.2):
For any 0 ≤ a ≤ b ≤ 1, let f([a,b]) denote the section of the space-filling curve f from f(a) to f(b). The box-to-curve ratio of a section f([a,b]) is: bcr(f([a,b])) = |bbox(f([a,b]))| / |f([a,b])| where |bbox(·)| is the volume of the minimum axis-aligned bounding box and |f([a,b])| is the d-dimensional Lebesgue measure (volume) of the curve section.
### Baseline/Optimal Value
Theoretical optimal: BCR = 1 (bounding box equals curve volume)
Practical lower bound: BCR ≥ 2 for 2D curves based on rectangular grids (proven in the 2010 paper)
Asymptotic lower bound: BCR ≥ 4 − O(1/2^d) for any curve using Hamiltonian paths on hypercubic grids
### Dimensions Analyzed
Both papers analyze dimensions d = 2 through d = 6 with computed values, and provide asymptotic bounds for d ≥ 7. Table 1 from the hyperorthogonal paper:
Curve	d=2	d=3	d=4	d=5	d=6	d≥7
Lower bound (face-continuous)	2.00	2.54	3.15	3.54	3.76	4−16/(2^d+3)
Self-similar hyperorthogonal, f(0)=(0,...,0)	2.40	3.11	3.53	3.76	3.88	≤4
Self-similar hyperorthogonal, f(0)=(1/3,...,1/3,0)	—	3.14	3.67	3.83	3.92	≤4
Butz-Moore	2.40	3.11	4.74	7.08	10.65	Ω(2^(d/2))
Comparisons
Hyperorthogonal well-folded curves vs. Butz-Moore: The new curves maintain BCR ≤ 4 in all dimensions, while Butz-Moore curves degrade exponentially (Ω(2^(d/2)))
For d=2: Hilbert's curve achieves BCR = 2.40, and the βΩ-curve achieves BCR = 2.22 (best known)
Key insight: Traditional locality measures (like WL₂) don't predict bounding-box quality well. Curves with good locality (like Sierpiński-Knopp with WL₂=4) can have poor BCR (3.0), while curves with moderate locality can have excellent BCR.
### Extent Sizes in Haverkort Papers
2010 Paper ("Locality and bounding-box quality of two-dimensional space-filling curves")
Used random sampling rather than exhaustive grid testing
Sample sizes: 500 to 18,000 points per test (log-uniform distribution)
Not a fixed grid like 32k × 32k
2015 Paper ("Hyperorthogonal well-folded Hilbert curves")
Uses approximating curves A_k with 2^(d·k) total points
For 2D (d=2): tested with k = 2 to 6, meaning:
k=2: 4×4 = 16 points
k=3: 8×8 = 64 points
k=4: 16×16 = 256 points
k=5: 32×32 = 1,024 points
k=6: 64×64 = 4,096 points
For higher dimensions (d=3 to 6), k was limited to 2 ≤ k ≤ 12/d:
d=3: up to 16×16×16 = 4,096 points
d=4: up to 8×8×8×8 = 4,096 points
d=5: up to 4×4×4×4×4 = 1,024 points
d=6: up to 4×4×4×4×4×4 = 4,096 points

**Answer to your question:** No, they did not use 32k × 32k grids. The largest 2D grid was 64×64 (4,096 points). The theoretical bounds they prove hold for arbitrary k (and thus arbitrarily large grids), but computational verification was done on these smaller sizes.

## Metrics in Gotsman & Lindenbaum (1996) "On the Metric Properties of Discrete Space-Filling Curves"
### Two Locality Metrics: L₁ and L₂
L₁(C) - Worst-case locality (maximum ratio) $$L_1(C) = \max_{i,j \in [N^m], i < j} \frac{d(C(i), C(j))^m}{|i - j|}$$ Measures the worst case where points close along the curve are far apart in Euclidean space. L₂(C) - Best-case locality (minimum ratio) $$L_2(C) = \min_{i,j \in [N^m], i < j} \frac{d(C(i), C(j))^m}{|i - j|}$$ Measures the best case (whether curve distance predicts Euclidean distance).
### Key Results
Curve Type	L₁ Bound	L₂ Bound
Any curve (lower bound)	L₁(C) ≥ 2^m - 1	L₂(C) = O(N^(1-m)) → 0
Raster scan	L₁ = Ω(N^(m-1)) — poor	—
Hilbert (general m-D)	L₁ ≤ (m+3)^(m/2) · 2^m	—
Hilbert 2D	6(1 - O(2^(-k))) ≤ L₁ ≤ 6⅔	—
Hilbert 3D	L₁ ≤ 23 (by simulation)	—
### Baseline
* Lower bound for any curve: L₁ ≥ 2^m - 1 (Theorem 1)
* For 2D: L₁ ≥ 3 (improved to 3.25 by exhaustive search)
* L₂ always goes to 0 as N→∞ (Theorem 2) — curve distance cannot reliably predict Euclidean distance
### Dimensions Analyzed
* General m-dimensional theory
* Specific results for 2D and 3D
* Grid sizes: N = 2^k (powers of 2 for Hilbert curves)
### Comparisons
*Hilbert curve: L₁ ≈ 6 for 2D — near optimal (lower bound is 3.25)
* Raster scan: L₁ = Ω(N^(m-1)) — grows with grid size, very poor
* Hilbert achieves constant L₁ independent of grid size
### Key Insight
The exponent m in the metrics normalizes for dimensionality. The paper proves Hilbert curves are near-optimal for preserving locality — points close along the curve stay within a bounded Euclidean distance, with that bound being a small constant (≈6 in 2D) regardless of grid size.

## Gotsman & Lindenbaum Locality Ratio

The two locality metrics from Gotsman & Lindenbaum are: L₁(C) - Maximum locality ratio:

L₁(C) = max over all pairs (i,j) of:  d(C(i), C(j))^m / |i - j|
L₂(C) - Minimum locality ratio:

L₂(C) = min over all pairs (i,j) of:  d(C(i), C(j))^m / |i - j|
Where:
i and j are positions along the curve (indices in [N^m])
C(i) and C(j) are the corresponding grid points
d(C(i), C(j)) is the Euclidean distance between those grid points
|i - j| is the distance along the curve
m is the dimension
In plain English:
L₁ finds the worst case: the pair of points closest along the curve but farthest apart in Euclidean space
L₂ finds the best case: the pair closest in Euclidean space relative to their curve distance
The exponent m normalizes for dimension since max Euclidean distance is O(N) while max curve distance is O(N^m).
